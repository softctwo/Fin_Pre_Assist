"""å‘é‡æœåŠ¡è¾¹ç•Œæµ‹è¯• - æå‡æµ‹è¯•è¦†ç›–ç‡"""
import pytest
import numpy as np
from unittest.mock import Mock, patch, MagicMock
from app.services.vector_service import VectorService


class TestVectorServiceBoundaryCases:
    """å‘é‡æœåŠ¡è¾¹ç•Œæ¡ä»¶æµ‹è¯•"""

    @pytest.fixture
    def vector_service(self):
        """åˆ›å»ºå‘é‡æœåŠ¡å®ä¾‹"""
        return VectorService()

    @pytest.fixture
    def sample_embeddings(self):
        """æä¾›æ ·æœ¬åµŒå…¥å‘é‡"""
        return {
            'text1': np.random.randn(1536).tolist(),
            'text2': np.random.randn(1536).tolist(),
            'text3': np.random.randn(1536).tolist(),
        }

    # ========== å‘é‡ç»´åº¦è¾¹ç•Œæµ‹è¯• ==========
    @pytest.mark.asyncio
    async def test_add_document_with_different_dimensions(self, vector_service):
        """æµ‹è¯•ä¸åŒç»´åº¦çš„å‘é‡"""
        # æ­£å¸¸ç»´åº¦
        normal_embedding = np.random.randn(1536).tolist()

        # è¾¹ç•Œç»´åº¦
        test_cases = [
            (np.random.randn(1).tolist(), "1ç»´å‘é‡"),
            (np.random.randn(100).tolist(), "100ç»´å‘é‡"),
            (np.random.randn(768).tolist(), "768ç»´å‘é‡(BERT)"),
            (np.random.randn(1024).tolist(), "1024ç»´å‘é‡"),
            (np.random.randn(1536).tolist(), "1536ç»´å‘é‡(OpenAI)"),
            (np.random.randn(2048).tolist(), "2048ç»´å‘é‡"),
            (np.random.randn(4096).tolist(), "4096ç»´å‘é‡"),
        ]

        for embedding, description in test_cases:
            try:
                await vector_service.add_document(
                    doc_id=f"test_{description}",
                    title="æµ‹è¯•æ ‡é¢˜",
                    content="æµ‹è¯•å†…å®¹",
                    embedding=embedding,
                    metadata={"type": description}
                )
                # å¦‚æœæ”¯æŒå¤šç»´åº¦ï¼Œåº”è¯¥æˆåŠŸ
            except (ValueError, RuntimeError) as e:
                # å¦‚æœä¸æ”¯æŒï¼Œåº”è¯¥ç»™å‡ºæ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
                assert "dimension" in str(e).lower() or "ç»´åº¦" in str(e)

    @pytest.mark.asyncio
    async def test_cosine_similarity_edge_cases(self, vector_service):
        """æµ‹è¯•ä½™å¼¦ç›¸ä¼¼åº¦è¾¹ç•Œæƒ…å†µ"""
        # ç›¸åŒå‘é‡
        vec1 = [1, 0, 0]
        vec2 = [1, 0, 0]
        similarity = vector_service._cosine_similarity(vec1, vec2)
        assert abs(similarity - 1.0) < 1e-10

        # ç›¸åå‘é‡
        vec3 = [1, 0, 0]
        vec4 = [-1, 0, 0]
        similarity = vector_service._cosine_similarity(vec3, vec4)
        assert abs(similarity - (-1.0)) < 1e-10

        # æ­£äº¤å‘é‡
        vec5 = [1, 0, 0]
        vec6 = [0, 1, 0]
        similarity = vector_service._cosine_similarity(vec5, vec6)
        assert abs(similarity - 0.0) < 1e-10

        # é›¶å‘é‡
        vec7 = [0, 0, 0]
        vec8 = [1, 1, 1]
        with pytest.raises((ValueError, ZeroDivisionError)):
            vector_service._cosine_similarity(vec7, vec8)

    @pytest.mark.asyncio
    async def test_euclidean_distance_edge_cases(self, vector_service):
        """æµ‹è¯•æ¬§å‡ é‡Œå¾—è·ç¦»è¾¹ç•Œæƒ…å†µ"""
        # ç›¸åŒç‚¹
        vec1 = [1, 2, 3]
        vec2 = [1, 2, 3]
        distance = vector_service._euclidean_distance(vec1, vec2)
        assert abs(distance - 0.0) < 1e-10

        # æœ€å¤§è·ç¦»ï¼ˆå¯¹è§’çº¿ï¼‰
        vec3 = [0, 0, 0]
        vec4 = [1, 1, 1]
        distance = vector_service._euclidean_distance(vec3, vec4)
        expected = np.sqrt(3)  # sqrt(1Â²+1Â²+1Â²)
        assert abs(distance - expected) < 1e-10

        # ä¸åŒç»´åº¦å‘é‡
        vec5 = [1, 2]
        vec6 = [1, 2, 3]
        with pytest.raises(ValueError):
            vector_service._euclidean_distance(vec5, vec6)

    # ========== å‘é‡æ•°å€¼è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_extreme_vector_values(self, vector_service):
        """æµ‹è¯•æç«¯å‘é‡å€¼"""
        extreme_cases = [
            ([float('inf')] * 1536, "æ­£æ— ç©·å¤§"),
            ([float('-inf')] * 1536, "è´Ÿæ— ç©·å¤§"),
            ([float('nan')] * 1536, "éæ•°å­—(NaN)"),
            ([1e308] * 1536, "æå¤§å€¼"),
            ([1e-308] * 1536, "æå°å€¼"),
            ([0.0] * 1536, "é›¶å‘é‡"),
            ([-0.0] * 1536, "è´Ÿé›¶å‘é‡"),
        ]

        for values, description in extreme_cases:
            try:
                normalized = vector_service._normalize_vector(values)
                # å¦‚æœèƒ½æ­£å¸¸å¤„ç†ï¼Œç»“æœåº”è¯¥æ˜¯æœ‰æ•ˆçš„
                assert len(normalized) == 1536
                assert all(not np.isnan(x) for x in normalized)
                assert all(not np.isinf(x) for x in normalized)
            except (ValueError, RuntimeError, OverflowError) as e:
                # å¦‚æœä¸èƒ½å¤„ç†ï¼Œåº”è¯¥ç»™å‡ºåˆé€‚çš„é”™è¯¯
                print(f"{description}å¤„ç†å¼‚å¸¸: {e}")

    @pytest.mark.asyncio
    async def test_vector_normalization_edge_cases(self, vector_service):
        """æµ‹è¯•å‘é‡å½’ä¸€åŒ–è¾¹ç•Œæƒ…å†µ"""
        # å·²ç»æ˜¯å•ä½å‘é‡
        unit_vec = [1, 0, 0]
        normalized = vector_service._normalize_vector(unit_vec)
        assert abs(np.linalg.norm(normalized) - 1.0) < 1e-10

        # é›¶å‘é‡
        zero_vec = [0, 0, 0]
        with pytest.raises((ValueError, ZeroDivisionError)):
            vector_service._normalize_vector(zero_vec)

        # éå¸¸å°çš„å‘é‡
        tiny_vec = [1e-100, 1e-100, 1e-100]
        normalized = vector_service._normalize_vector(tiny_vec)
        assert abs(np.linalg.norm(normalized) - 1.0) < 1e-10

    # ========== æœç´¢å‚æ•°è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_search_with_extreme_k_values(self, vector_service, sample_embeddings):
        """æµ‹è¯•æç«¯çš„kå€¼æœç´¢"""
        # å…ˆæ·»åŠ ä¸€äº›æ–‡æ¡£
        for doc_id, embedding in sample_embeddings.items():
            await vector_service.add_document(doc_id, f"å†…å®¹{doc_id}", "å†…å®¹", embedding=embedding)

        # æµ‹è¯•ä¸åŒçš„kå€¼
        test_cases = [
            (0, "k=0"),
            (1, "k=1"),
            (len(sample_embeddings), "k=æ–‡æ¡£æ€»æ•°"),
            (len(sample_embeddings) + 1, "k>æ–‡æ¡£æ€»æ•°"),
            (1000, "k=1000(è¿œå¤§äºæ–‡æ¡£æ•°)"),
            (-1, "k=-1(è´Ÿæ•°)"),
        ]

        query_embedding = np.random.randn(1536).tolist()

        for k, description in test_cases:
            try:
                results = await vector_service.search_documents(query_embedding, n_results=k)
                assert isinstance(results, list)

                if k <= 0:
                    assert len(results) == 0
                elif k > len(sample_embeddings):
                    assert len(results) == len(sample_embeddings)
                else:
                    assert len(results) == k

            except (ValueError, RuntimeError) as e:
                # è´Ÿæ•°kå€¼åº”è¯¥æŠ›å‡ºå¼‚å¸¸
                if k < 0:
                    assert "k" in str(e).lower() or "invalid" in str(e).lower()

    @pytest.mark.asyncio
    async def test_search_similarity_threshold_boundary(self, vector_service, sample_embeddings):
        """æµ‹è¯•ç›¸ä¼¼åº¦é˜ˆå€¼è¾¹ç•Œ"""
        # æ·»åŠ å®Œå…¨ç›¸åŒçš„æ–‡æ¡£ç”¨äºæµ‹è¯•
        identical_embedding = [0.1] * 1536
        await vector_service.add_document("doc1", "ç›¸åŒå†…å®¹1", "ç›¸åŒå†…å®¹1", embedding=identical_embedding)
        await vector_service.add_document("doc2", "ç›¸åŒå†…å®¹2", "ç›¸åŒå†…å®¹2", embedding=identical_embedding)

        test_cases = [
            (-1.0, "é˜ˆå€¼=-1.0"),
            (0.0, "é˜ˆå€¼=0.0"),
            (0.5, "é˜ˆå€¼=0.5"),
            (1.0, "é˜ˆå€¼=1.0"),
            (1.5, "é˜ˆå€¼=1.5(å¤§äº1)"),
            (-0.5, "é˜ˆå€¼=-0.5(å°äº-1)"),
        ]

        for threshold, description in test_cases:
            results = await vector_service.search_documents(
                "",
                n_results=10,
                filter_metadata={"distance": {"$gte": threshold}}
            )

            if threshold <= 1.0:  # åˆç†çš„é˜ˆå€¼åº”è¯¥è¿”å›ç»“æœ
                assert len(results) >= 1
            else:  # è¿‡é«˜çš„é˜ˆå€¼å¯èƒ½æ²¡æœ‰ç»“æœ
                assert len(results) >= 0

    # ========== æ–‡æ¡£å†…å®¹è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_add_empty_document(self, vector_service):
        """æµ‹è¯•ç©ºæ–‡æ¡£"""
        empty_embedding = [0.0] * 1536

        await vector_service.add_document(
            doc_id="empty_doc",
            title="ç©ºæ–‡æ¡£",
            content="",
            embedding=empty_embedding,
            metadata={"type": "empty"}
        )

        # åº”è¯¥èƒ½æ­£å¸¸æœç´¢åˆ°
        results = await vector_service.search_documents("", n_results=1)
        assert len(results) >= 1
        assert results[0]['id'].startswith("doc_empty_doc")

    @pytest.mark.asyncio
    async def test_add_very_long_document(self, vector_service):
        """æµ‹è¯•è¶…é•¿æ–‡æ¡£"""
        # åˆ›å»ºå¾ˆé•¿çš„å†…å®¹
        long_content = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„å†…å®¹ã€‚" * 10000  # çº¦20ä¸‡å­—
        embedding = np.random.randn(1536).tolist()

        await vector_service.add_document(
            doc_id="long_doc",
            title="è¶…é•¿æ–‡æ¡£",
            content=long_content,
            embedding=embedding,
            metadata={"type": "long_content", "length": len(long_content)}
        )

        results = await vector_service.search_documents("", n_results=1)
        assert len(results) >= 1
        assert results[0]['id'].startswith("doc_long_doc")

    @pytest.mark.asyncio
    async def test_add_document_with_special_characters(self, vector_service):
        """æµ‹è¯•åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡æ¡£"""
        special_contents = [
            "å†…å®¹åŒ…å«\næ¢è¡Œ\r\nå­—ç¬¦",
            "å†…å®¹åŒ…å«\tåˆ¶è¡¨ç¬¦",
            "å†…å®¹åŒ…å«\"å¼•å·\"å’Œ'å•å¼•å·'",
            "å†…å®¹åŒ…å«\\åæ–œæ \\",
            "å†…å®¹åŒ…å«ğŸš€ Emoji ğŸ¯",
            "å†…å®¹åŒ…å«\u0000ç©ºå­—ç¬¦",
            "å†…å®¹åŒ…å«HTML: <div>æµ‹è¯•</div>",
            "å†…å®¹åŒ…å«JSON: {\"key\": \"value\"}",
            "å†…å®¹åŒ…å«XML: <root>æ•°æ®</root>",
        ]

        for i, content in enumerate(special_contents):
            embedding = np.random.randn(1536).tolist()
            doc_id = f"special_doc_{i}"

            await vector_service.add_document(
                doc_id=doc_id,
                title=f"ç‰¹æ®Šå­—ç¬¦æ–‡æ¡£{i}",
                content=content,
                embedding=embedding,
                metadata={"type": "special_chars", "index": i}
            )

            # éªŒè¯èƒ½æ­£å¸¸æœç´¢
            results = await vector_service.search_documents("", n_results=1)
            assert len(results) >= 1
            assert results[0]['id'].startswith(f"doc_{doc_id}")

    # ========== å…ƒæ•°æ®è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_add_document_with_extreme_metadata(self, vector_service):
        """æµ‹è¯•æç«¯çš„å…ƒæ•°æ®"""
        embedding = np.random.randn(1536).tolist()

        # å¤§å…ƒæ•°æ®
        large_metadata = {f"key_{i}": f"value_{i}" for i in range(1000)}
        await vector_service.add_document(
            doc_id="large_metadata_doc",
            title="å¤§å…ƒæ•°æ®æµ‹è¯•",
            content="å¤§å…ƒæ•°æ®æµ‹è¯•",
            embedding=embedding,
            metadata=large_metadata
        )

        # åµŒå¥—å…ƒæ•°æ®
        nested_metadata = {
            "level1": {
                "level2": {
                    "level3": {
                        "level4": "deep_value"
                    }
                }
            }
        }
        await vector_service.add_document(
            doc_id="nested_metadata_doc",
            title="åµŒå¥—å…ƒæ•°æ®æµ‹è¯•",
            content="åµŒå¥—å…ƒæ•°æ®æµ‹è¯•",
            embedding=embedding,
            metadata=nested_metadata
        )

        # ç‰¹æ®Šå€¼å…ƒæ•°æ®
        special_metadata = {
            "null_value": None,
            "empty_string": "",
            "large_number": 1e308,
            "small_number": 1e-308,
            "boolean_true": True,
            "boolean_false": False,
            "empty_list": [],
            "empty_dict": {},
        }
        await vector_service.add_document(
            doc_id="special_metadata_doc",
            title="ç‰¹æ®Šå…ƒæ•°æ®æµ‹è¯•",
            content="ç‰¹æ®Šå…ƒæ•°æ®æµ‹è¯•",
            embedding=embedding,
            metadata=special_metadata
        )

        # éªŒè¯æ‰€æœ‰æ–‡æ¡£éƒ½èƒ½è¢«æœç´¢åˆ°
        results = await vector_service.search_documents("", n_results=10)
        doc_ids = [result['id'] for result in results]
        assert any(doc_id.startswith("doc_large_metadata_doc") for doc_id in doc_ids)
        assert any(doc_id.startswith("doc_nested_metadata_doc") for doc_id in doc_ids)
        assert any(doc_id.startswith("doc_special_metadata_doc") for doc_id in doc_ids)

    # ========== æ‰¹é‡æ“ä½œè¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_batch_add_documents_extreme_cases(self, vector_service):
        """æµ‹è¯•æ‰¹é‡æ·»åŠ æ–‡æ¡£çš„æç«¯æƒ…å†µ"""
        # ç©ºæ‰¹æ¬¡
        await vector_service.batch_add_documents([])

        # å¤§æ‰¹é‡
        large_batch = []
        for i in range(1000):
            large_batch.append({
                'doc_id': f"batch_doc_{i}",
                'title': f"æ‰¹é‡æ–‡æ¡£{i}",
                'content': f"æ‰¹é‡æ–‡æ¡£å†…å®¹ {i}",
                'embedding': np.random.randn(1536).tolist(),
                'metadata': {'batch_index': i}
            })

        try:
            await vector_service.batch_add_documents(large_batch)

            # éªŒè¯éƒ¨åˆ†æ–‡æ¡£èƒ½è¢«æœç´¢åˆ°
            sample_embedding = large_batch[0]['embedding']
            results = await vector_service.search_documents("", n_results=10)
            assert len(results) >= 1

        except (MemoryError, RuntimeError) as e:
            # å†…å­˜ä¸è¶³çš„æƒ…å†µä¸‹åº”è¯¥ä¼˜é›…å¤„ç†
            print(f"å¤§æ‰¹é‡å¤„ç†éœ€è¦ä¼˜åŒ–: {e}")

    @pytest.mark.asyncio
    async def test_batch_delete_documents_edge_cases(self, vector_service, sample_embeddings):
        """æµ‹è¯•æ‰¹é‡åˆ é™¤æ–‡æ¡£çš„è¾¹ç•Œæƒ…å†µ"""
        # å…ˆæ·»åŠ æ–‡æ¡£
        for doc_id, embedding in sample_embeddings.items():
            await vector_service.add_document(doc_id, f"å†…å®¹{doc_id}", "å†…å®¹", embedding=embedding)

        # åˆ é™¤ç©ºåˆ—è¡¨
        await vector_service.delete_document(doc_id=None)

        # åˆ é™¤ä¸å­˜åœ¨çš„æ–‡æ¡£ID
        await vector_service.delete_document(doc_id=-1)

        # åˆ é™¤æ··åˆåˆ—è¡¨ï¼ˆå­˜åœ¨å’Œä¸å­˜åœ¨ï¼‰
        await vector_service.delete_document(doc_id="text1")
        await vector_service.delete_document(doc_id="text2")


        # éªŒè¯å­˜åœ¨çš„æ–‡æ¡£è¢«åˆ é™¤
        remaining_docs = ["text3"]  # å‡è®¾åªåˆ é™¤äº†text1å’Œtext2
        for doc_id in remaining_docs:
            results = await vector_service.search_documents(sample_embeddings[doc_id], n_results=10)
            doc_ids = [result['id'] for result in results]
            assert any(d.startswith(f"doc_{doc_id}") for d in doc_ids)

    # ========== ç›¸ä¼¼åº¦è®¡ç®—è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_similarity_with_identical_vectors(self, vector_service):
        """æµ‹è¯•å®Œå…¨ç›¸åŒå‘é‡çš„ç›¸ä¼¼åº¦"""
        vec1 = [0.1, 0.2, 0.3, 0.4, 0.5] + [0.0] * (1536 - 5)
        vec2 = [0.1, 0.2, 0.3, 0.4, 0.5] + [0.0] * (1536 - 5)

        cosine_sim = vector_service._cosine_similarity(vec1, vec2)
        euclidean_dist = vector_service._euclidean_distance(vec1, vec2)

        assert abs(cosine_sim - 1.0) < 1e-10
        assert abs(euclidean_dist - 0.0) < 1e-10

    @pytest.mark.asyncio
    async def test_similarity_with_orthogonal_vectors(self, vector_service):
        """æµ‹è¯•æ­£äº¤å‘é‡çš„ç›¸ä¼¼åº¦"""
        # åˆ›å»ºæ­£äº¤å‘é‡
        vec1 = [1.0] + [0.0] * (1536 - 1)
        vec2 = [0.0] + [1.0] + [0.0] * (1536 - 2)

        cosine_sim = vector_service._cosine_similarity(vec1, vec2)
        assert abs(cosine_sim - 0.0) < 1e-10

    @pytest.mark.asyncio
    async def test_similarity_with_opposite_vectors(self, vector_service):
        """æµ‹è¯•ç›¸åå‘é‡çš„ç›¸ä¼¼åº¦"""
        vec1 = [0.1, 0.2, 0.3] + [0.0] * (1536 - 3)
        vec2 = [-0.1, -0.2, -0.3] + [0.0] * (1536 - 3)

        cosine_sim = vector_service._cosine_similarity(vec1, vec2)
        assert abs(cosine_sim - (-1.0)) < 1e-10

    # ========== å†…å­˜å’Œæ€§èƒ½è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_memory_efficiency_with_large_index(self, vector_service):
        """æµ‹è¯•å¤§ç´¢å¼•çš„å†…å­˜æ•ˆç‡"""
        # æ·»åŠ å¤§é‡æ–‡æ¡£
        large_embeddings = {}
        for i in range(10000):
            doc_id = f"memory_test_{i}"
            embedding = np.random.randn(1536).tolist()
            large_embeddings[doc_id] = embedding

            await vector_service.add_document(
                doc_id=doc_id,
                title=f"å†…å­˜æµ‹è¯•æ–‡æ¡£{i}",
                content=f"å†…å­˜æµ‹è¯•æ–‡æ¡£ {i}",
                embedding=embedding,
                metadata={'test_id': i}
            )

            # æ¯1000ä¸ªæ–‡æ¡£æ£€æŸ¥ä¸€æ¬¡å†…å­˜æ•ˆç‡
            if i % 1000 == 0 and i > 0:
                # æ‰§è¡Œæœç´¢æµ‹è¯•æ€§èƒ½
                query_embedding = large_embeddings[f"memory_test_{i-1}"]
                results = await vector_service.search_documents("", n_results=10)
                assert len(results) >= 1

                print(f"å·²æ·»åŠ  {i+1} ä¸ªæ–‡æ¡£ï¼Œæœç´¢æ­£å¸¸")

    @pytest.mark.asyncio
    async def test_search_performance_with_large_dataset(self, vector_service):
        """æµ‹è¯•å¤§æ•°æ®é›†çš„æœç´¢æ€§èƒ½"""
        import time

        # æ·»åŠ æµ‹è¯•æ•°æ®
        test_embeddings = []
        for i in range(1000):
            embedding = np.random.randn(1536).tolist()
            test_embeddings.append(embedding)

            await vector_service.add_document(
                doc_id=f"perf_doc_{i}",
                title=f"æ€§èƒ½æµ‹è¯•æ–‡æ¡£{i}",
                content=f"æ€§èƒ½æµ‹è¯•æ–‡æ¡£ {i}",
                embedding=embedding
            )

        # æµ‹è¯•æœç´¢æ€§èƒ½
        query_embedding = test_embeddings[0]

        # é¢„çƒ­
        await vector_service.search_documents("", n_results=10)

        # æ­£å¼æµ‹è¯•
        start_time = time.time()
        results = await vector_service.search_documents("", n_results=10)
        end_time = time.time()

        search_time = end_time - start_time

        assert len(results) >= 1
        assert search_time < 1.0, f"æœç´¢æ—¶é—´è¿‡é•¿: {search_time}ç§’"
        print(f"1000æ–‡æ¡£æœç´¢è€—æ—¶: {search_time:.4f}ç§’")

    # ========== é”™è¯¯å¤„ç†å’Œå¼‚å¸¸è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_search_with_invalid_query_vector(self, vector_service):
        """æµ‹è¯•æ— æ•ˆçš„æŸ¥è¯¢å‘é‡"""
        invalid_vectors = [
            None,
            "not a vector",
            [],  # ç©ºå‘é‡
            [1, 2, 3],  # ç»´åº¦ä¸åŒ¹é…
            [[1, 2], [3, 4]],  # äºŒç»´æ•°ç»„
            {1: 2, 3: 4},  # å­—å…¸
        ]

        for invalid_vector in invalid_vectors:
            with pytest.raises((ValueError, TypeError, AttributeError)):
                await vector_service.search_documents(invalid_vector, n_results=5)

    @pytest.mark.asyncio
    async def test_add_document_with_invalid_embedding(self, vector_service):
        """æµ‹è¯•æ·»åŠ æ–‡æ¡£æ—¶æ— æ•ˆçš„åµŒå…¥å‘é‡"""
        invalid_embeddings = [
            None,
            "not embedding",
            [],  # ç©ºå‘é‡
            [1, 2, 3],  # ç»´åº¦ä¸åŒ¹é…
            "1,2,3,4,5",  # å­—ç¬¦ä¸²å½¢å¼çš„å‘é‡
            [[1, 2], [3, 4]],  # é”™è¯¯çš„ç»´åº¦
        ]

        for invalid_embedding in invalid_embeddings:
            with pytest.raises((ValueError, TypeError)):
                await vector_service.add_document(
                    doc_id="test_doc",
                    title="æµ‹è¯•æ ‡é¢˜",
                    content="æµ‹è¯•å†…å®¹",
                    embedding=invalid_embedding
                )

    # ========== å¹¶å‘è®¿é—®è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_concurrent_vector_operations(self, vector_service):
        """æµ‹è¯•å¹¶å‘å‘é‡æ“ä½œ"""
        import asyncio

        results = {}
        errors = {}

        async def add_document_task(task_id):
            try:
                embedding = np.random.randn(1536).tolist()
                doc_id = f"concurrent_doc_{task_id}"

                await vector_service.add_document(
                    doc_id=doc_id,
                    title=f"å¹¶å‘æµ‹è¯•æ–‡æ¡£{task_id}",
                    content=f"å¹¶å‘æµ‹è¯•æ–‡æ¡£ {task_id}",
                    embedding=embedding
                )

                # ç«‹å³æœç´¢
                search_results = await vector_service.search_documents("", n_results=1)
                results[task_id] = len(search_results) > 0

            except Exception as e:
                errors[task_id] = str(e)

        # å¯åŠ¨å¤šä¸ªä»»åŠ¡
        tasks = [add_document_task(i) for i in range(10)]
        await asyncio.gather(*tasks)

        # éªŒè¯ç»“æœ
        assert len(results) > 0, "åº”è¯¥æœ‰æˆåŠŸçš„å¹¶å‘æ“ä½œ"
        assert len(errors) == 0, f"ä¸åº”è¯¥æœ‰é”™è¯¯: {errors}"
        assert all(results.values()), "æ‰€æœ‰å¹¶å‘æ“ä½œéƒ½åº”è¯¥æˆåŠŸ"

    # ========== æ•°æ®ä¸€è‡´æ€§è¾¹ç•Œæµ‹è¯• ==========

    @pytest.mark.asyncio
    async def test_vector_precision_preservation(self, vector_service):
        """æµ‹è¯•å‘é‡ç²¾åº¦ä¿æŒ"""
        # é«˜ç²¾åº¦å‘é‡
        high_precision_vector = [1.123456789012345] * 1536

        await vector_service.add_document(
            doc_id="precision_test",
            title="ç²¾åº¦æµ‹è¯•",
            content="ç²¾åº¦æµ‹è¯•",
            embedding=high_precision_vector
        )

        # æœç´¢å¹¶éªŒè¯ç²¾åº¦
        results = await vector_service.search_documents("", n_results=1)

        assert len(results) > 0
        assert results[0]['id'].startswith("doc_precision_test")
        # ç›¸ä¼¼åº¦åº”è¯¥æ¥è¿‘1.0
        assert results[0]['distance'] > 0.999999

    @pytest.mark.asyncio
    async def test_metadata_persistence(self, vector_service):
        """æµ‹è¯•å…ƒæ•°æ®æŒä¹…æ€§"""
        complex_metadata = {
            "nested": {
                "deep": {
                    "value": "test"
                }
            },
            "list": [1, 2, 3, "four", {"five": 5}],
            "unicode": "ä¸­æ–‡æµ‹è¯• ğŸš€",
            "null_value": None,
            "empty_string": "",
            "number": 42.42,
            "boolean": True
        }

        embedding = np.random.randn(1536).tolist()

        await vector_service.add_document(
            doc_id="metadata_test",
            title="å…ƒæ•°æ®æµ‹è¯•",
            content="å…ƒæ•°æ®æµ‹è¯•",
            embedding=embedding,
            metadata=complex_metadata
        )

        # éªŒè¯æœç´¢è¿”å›çš„å…ƒæ•°æ®
        results = await vector_service.search_documents("", n_results=1)

        assert len(results) > 0
        returned_metadata = results[0]['metadata'] 

        # éªŒè¯å¤æ‚æ•°æ®ç»“æ„
        assert returned_metadata["nested"]["deep"]["value"] == "test"
        assert returned_metadata["list"] == [1, 2, 3, "four", {"five": 5}]
        assert returned_metadata["unicode"] == "ä¸­æ–‡æµ‹è¯• ğŸš€"
        assert returned_metadata["null_value"] is None
        assert returned_metadata["empty_string"] == ""
        assert returned_metadata["number"] == 42.42
        assert returned_metadata["boolean"] is True